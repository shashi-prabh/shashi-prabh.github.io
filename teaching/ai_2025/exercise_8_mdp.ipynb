{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "from notebook import psource, pseudocode, plot_pomdp_utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c549e",
   "metadata": {},
   "source": [
    "## Sequential Decision Problems\n",
    "\n",
    "Now that we have the tools required to solve MDPs, let us see how Sequential Decision Problems can be solved step by step and how a few built-in tools in the GridMDP class help us better analyse the problem at hand. \n",
    "As always, we will work with the grid world from **Figure 17.1** from the book.\n",
    "![title](images/grid_mdp.jpg)\n",
    "<br>This is the environment for our agent.\n",
    "We assume for now that the environment is _fully observable_, so that the agent always knows where it is.\n",
    "We also assume that the transitions are **Markovian**, that is, the probability of reaching state $s'$ from state $s$ depends only on $s$ and not on the history of earlier states.\n",
    "Almost all stochastic decision problems can be reframed as a Markov Decision Process just by tweaking the definition of a _state_ for that particular problem.\n",
    "<br>\n",
    "However, the actions of our agent in this environment are unreliable. In other words, the motion of our agent is stochastic. \n",
    "<br><br>\n",
    "More specifically, the agent may - \n",
    "* move correctly in the intended direction with a probability of _0.8_,  \n",
    "* move $90^\\circ$ to the right of the intended direction with a probability 0.1\n",
    "* move $90^\\circ$ to the left of the intended direction with a probability 0.1\n",
    "<br><br>\n",
    "The agent stays put if it bumps into a wall.\n",
    "![title](images/grid_mdp_agent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e50c12",
   "metadata": {},
   "source": [
    "To completely define our task environment, we need to specify the utility function for the agent. \n",
    "This is the function that gives the agent a rough estimate of how good being in a particular state is, or how much _reward_ an agent receives by being in that state.\n",
    "The agent then tries to maximize the reward it gets.\n",
    "As the decision problem is sequential, the utility function will depend on a sequence of states rather than on a single state.\n",
    "For now, we simply stipulate that in each state $s$, the agent receives a finite reward $R(s)$.\n",
    "\n",
    "For any given state, the actions the agent can take are encoded as given below:\n",
    "- Move Up: (0, 1)\n",
    "- Move Down: (0, -1)\n",
    "- Move Left: (-1, 0)\n",
    "- Move Right: (1, 0)\n",
    "- Do nothing: `None`\n",
    "\n",
    "We now wonder what a valid solution to the problem might look like. \n",
    "We cannot have fixed action sequences as the environment is stochastic and we can eventually end up in an undesirable state.\n",
    "Therefore, a solution must specify what the agent shoulddo for _any_ state the agent might reach.\n",
    "<br>\n",
    "Such a solution is known as a **policy** and is usually denoted by $\\pi$.\n",
    "<br>\n",
    "The **optimal policy** is the policy that yields the highest expected utility an is usually denoted by $\\pi^*$.\n",
    "<br>\n",
    "The `GridMDP` class has a useful method `to_arrows` that outputs a grid showing the direction the agent should move, given a policy.\n",
    "We will use this later to better understand the properties of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317416f",
   "metadata": {},
   "source": [
    "### Example Case\n",
    "---\n",
    "R(s) = -0.04 in all states except terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e798c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this environment is also initialized in mdp.py by default\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, None, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(3, 2), (3, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4eac2b",
   "metadata": {},
   "source": [
    "We will use the `best_policy` function to find the best policy for this environment.\n",
    "But, as you can see, `best_policy` requires a utility function as well.\n",
    "We already know that the utility function can be found by `value_iteration`.\n",
    "Hence, our best policy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754461f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = best_policy(sequential_decision_environment, value_iteration(sequential_decision_environment, .001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a7980",
   "metadata": {},
   "source": [
    "We can now use the `to_arrows` method to see how our agent should pick its actions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a69d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n"
     ]
    }
   ],
   "source": [
    "from utils import print_table\n",
    "print_table(sequential_decision_environment.to_arrows(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934bf5d",
   "metadata": {},
   "source": [
    "This is exactly the output we expected\n",
    "<br>\n",
    "![title](images/-0.04.jpg)\n",
    "<br>\n",
    "Notice that, because the cost of taking a step is fairly small compared with the penalty for ending up in `(4, 2)` by accident, the optimal policy is conservative. \n",
    "In state `(3, 1)` it recommends taking the long way round, rather than taking the shorter way and risking getting a large negative reward of -1 in `(4, 2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4252ea3",
   "metadata": {},
   "source": [
    "Implement the following cases:\n",
    "1. R(s) = -0.4 in all states except in terminal states\n",
    "2. R(s) = -4 in all states except in terminal states\n",
    "3. R(s) = 4 in all states except in terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
